{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os"
      ],
      "metadata": {
        "id": "qBF6I2YkkAVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "FUT6gZlMkAgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.11/dist-packages/pyspark\""
      ],
      "metadata": {
        "id": "iGA_q1bFkAjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "0ez6RtB7kAmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bring in Pyspark functions into your session\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "p6foQr3xk0at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start our Pyspark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "su8z_OK4kjhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\tReading and Writing Data"
      ],
      "metadata": {
        "id": "aZFygDc1FiDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create simple dataframe\n",
        "df = spark.createDataFrame([(1, \"foo\"), (2, \"bar\"), (1, \"foo\")], [\"id\", \"value\"])\n",
        "df.show()"
      ],
      "metadata": {
        "id": "VPKFDTy-kApF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydata = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"original.csv\")"
      ],
      "metadata": {
        "id": "OjkIo5vxkAsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import json file\n",
        "\n",
        "import json\n",
        "df2 = spark.read.json('people.json')\n",
        "df2.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tm_PgHUrFmJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HB-ZU3NOk7rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Checking/cleaning & *Validation*"
      ],
      "metadata": {
        "id": "znygJjt1MtmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata.printSchema()"
      ],
      "metadata": {
        "id": "KV3UzBWidxor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploratory Functions\n",
        "\n",
        "\n",
        "df.describe()\n",
        "\n"
      ],
      "metadata": {
        "id": "rgEE6Qm-FYOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.summary()\n"
      ],
      "metadata": {
        "id": "TqaI54bf6XnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydata.columns\n"
      ],
      "metadata": {
        "id": "pMPiQ5X06XYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.dtypes"
      ],
      "metadata": {
        "id": "eKoN4TrAk7ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Null Checking & Missing Value Analysis\n",
        "mydata.select([\n",
        "    sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
        "    for c in mydata.columns\n",
        "]).show()\n",
        "\n",
        "\n",
        "mydata.filter(mydata.col.isNull())\n",
        "#Drop Rows with Any Null\n",
        "mydata.na.drop()\n",
        "mydata.na.fill(0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B5T0_-QNk7jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace Nulls Only in Selected Columns\n",
        "\n",
        "df_filled = df2.fillna(0, subset=['age'])\n",
        "df_filled .show()"
      ],
      "metadata": {
        "id": "yHQYNeWLk7f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate Detection\n",
        "\n",
        "\n",
        "df.groupBy(df.columns) \\\n",
        "  .count() \\\n",
        "  .filter(\"count > 1\") \\\n",
        "  .show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df.dropDuplicates().show()\n",
        "\n",
        "df_drop= df.dropDuplicates()\n",
        "df_drop.show()"
      ],
      "metadata": {
        "id": "VqJQ3rcZM126"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rp6pUzVUM10C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEskwJLYM1pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Mining & Feature Engineering"
      ],
      "metadata": {
        "id": "dDlvR1EINH-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering and Conditions\n",
        "filter()\n",
        "\n"
      ],
      "metadata": {
        "id": "io69HNNENMLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chained conditions using &, |, ~"
      ],
      "metadata": {
        "id": "nk9LgqOUh_HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n"
      ],
      "metadata": {
        "id": "dGzT1mMrNO2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Alice\", 29),\n",
        "    (2, \"Bob\", 35),\n",
        "    (3, \"Eve\", 25),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
        "\n",
        "# Simple condition: age > 30 OR name == \"Eve\"\n",
        "filtered_df = df.filter((col(\"age\") > 30) | (col(\"name\") == \"Eve\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "LUfMtOuMiTj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.show()"
      ],
      "metadata": {
        "id": "VZdHFgGViWpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark, you can use chained conditions with the bitwise operators:\n",
        "•\t& for AND\n",
        "•\t| for OR\n",
        "•\t~ for NOT\n",
        "These are used inside filter() or where() clauses, and each condition must be enclosed in parentheses to avoid operator precedence issues.\n"
      ],
      "metadata": {
        "id": "e5GTuqtrkmEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n"
      ],
      "metadata": {
        "id": "6eqghLAQiX4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Alice\", 29),\n",
        "    (2, \"Bob\", 35),\n",
        "    (3, \"Charlie\", 30),\n",
        "    (4, \"Diana\", 40),\n",
        "    (5, \"Eve\", 25)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "XLSxJMAQiXv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chained filter with &, |, ~\n",
        "# Show me those who are over 30 years of age and excluded anyone named Diana,\n",
        "filtered_df = df.filter(\n",
        "    ((col(\"age\") > 30) & (col(\"name\") != \"Diana\"))\n",
        ")\n",
        "\n",
        "# Show result\n",
        "filtered_df.show()\n",
        "\n",
        "filtered_df = df.filter(\n",
        "    ((col(\"age\") > 30) & (col(\"name\") != \"Diana\")) | (~(col(\"age\") < 30))\n",
        ")\n",
        "\n",
        "# Show result\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "id": "OC2RUiBtkw6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
