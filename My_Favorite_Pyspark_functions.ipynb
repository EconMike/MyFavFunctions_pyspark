{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os"
      ],
      "metadata": {
        "id": "qBF6I2YkkAVu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "FUT6gZlMkAgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.11/dist-packages/pyspark\""
      ],
      "metadata": {
        "id": "iGA_q1bFkAjV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ez6RtB7kAmV",
        "outputId": "9d04559f-a873-4181-f595-970daaa73ded"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.7-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.7-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bring in Pyspark functions into your session\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "p6foQr3xk0at"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "su8z_OK4kjhl",
        "outputId": "661713d1-366f-4f93-da1d-fcd033d3dc94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7aea69dd1bd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9f1985619684:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "â€¢\tReading and Writing Data"
      ],
      "metadata": {
        "id": "aZFygDc1FiDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create simple dataframe\n",
        "df = spark.createDataFrame([(1, \"foo\"), (2, \"bar\")], [\"id\", \"value\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPKFDTy-kApF",
        "outputId": "3cde4ad8-1158-4844-d2b1-81ce104f02b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id|value|\n",
            "+---+-----+\n",
            "|  1|  foo|\n",
            "|  2|  bar|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mydata = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"original.csv\")"
      ],
      "metadata": {
        "id": "OjkIo5vxkAsF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import json file\n",
        "\n",
        "import json\n",
        "df2 = spark.read.json('people.json')\n",
        "df2.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm_PgHUrFmJ8",
        "outputId": "90e559cb-5f9b-4c42-dd30-d90a51cba84c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|NULL|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HB-ZU3NOk7rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Checking/cleaning & *Validation*"
      ],
      "metadata": {
        "id": "znygJjt1MtmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV3UzBWidxor",
        "outputId": "11d39eb0-5a69-4e2b-a14f-87c3a70e4a3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- JobTitle: string (nullable = true)\n",
            " |-- Salary: string (nullable = true)\n",
            " |-- Latitude: string (nullable = true)\n",
            " |-- Longitude: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploratory Functions\n",
        "\n",
        "\n",
        "df.describe()\n",
        "df.summary()\n",
        "mydata.columns\n",
        "df2.dtypes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgEE6Qm-FYOG",
        "outputId": "df55fd23-c5d3-46b5-9395-ede7a70aa95e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, id: string, value: string]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKoN4TrAk7ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Null Checking & Missing Value Analysis\n",
        "mydata.select([\n",
        "    sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
        "    for c in mydata.columns\n",
        "]).show()\n",
        "\n",
        "\n",
        "mydata.filter(mydata.col.isNull())\n",
        "#Drop Rows with Any Null\n",
        "mydata.na.drop()\n",
        "mydata.na.fill(0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B5T0_-QNk7jV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3becca45-2b0a-4cd8-f5db-994776a12f91"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: string, first_name: string, last_name: string, gender: string, City: string, JobTitle: string, Salary: string, Latitude: string, Longitude: string]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace Nulls Only in Selected Columns\n",
        "\n",
        "df_filled = df2.fillna(0, subset=['age'])\n",
        "df_filled .show()"
      ],
      "metadata": {
        "id": "yHQYNeWLk7f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80cd807-ec5c-4b22-ccf4-6a6ad2ffd383"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "|age|   name|\n",
            "+---+-------+\n",
            "|  0|Michael|\n",
            "| 30|   Andy|\n",
            "| 19| Justin|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate Detection\n",
        "mydata.dropDuplicates()\n",
        "\n",
        "mydata.groupBy(mydata.columns) \\\n",
        "  .count() \\\n",
        "  .filter(\"count > 1\") \\\n",
        "  .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqJQ3rcZM126",
        "outputId": "bc88e734-a6fd-4b11-ab50-52a04a5d96e6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+------+----+--------+------+--------+---------+-----+\n",
            "| id|first_name|last_name|gender|City|JobTitle|Salary|Latitude|Longitude|count|\n",
            "+---+----------+---------+------+----+--------+------+--------+---------+-----+\n",
            "+---+----------+---------+------+----+--------+------+--------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rp6pUzVUM10C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEskwJLYM1pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Mining & Feature Engineering"
      ],
      "metadata": {
        "id": "dDlvR1EINH-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering and Conditions\n",
        "filter()\n",
        "where()\n"
      ],
      "metadata": {
        "id": "io69HNNENMLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chained conditions using &, |, ~"
      ],
      "metadata": {
        "id": "nk9LgqOUh_HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n"
      ],
      "metadata": {
        "id": "dGzT1mMrNO2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Alice\", 29),\n",
        "    (2, \"Bob\", 35),\n",
        "    (3, \"Eve\", 25),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
        "\n",
        "# Simple condition: age > 30 OR name == \"Eve\"\n",
        "filtered_df = df.filter((col(\"age\") > 30) | (col(\"name\") == \"Eve\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "LUfMtOuMiTj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.show()"
      ],
      "metadata": {
        "id": "VZdHFgGViWpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark, you can use chained conditions with the bitwise operators:\n",
        "â€¢\t& for AND\n",
        "â€¢\t| for OR\n",
        "â€¢\t~ for NOT\n",
        "These are used inside filter() or where() clauses, and each condition must be enclosed in parentheses to avoid operator precedence issues.\n"
      ],
      "metadata": {
        "id": "e5GTuqtrkmEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n"
      ],
      "metadata": {
        "id": "6eqghLAQiX4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Alice\", 29),\n",
        "    (2, \"Bob\", 35),\n",
        "    (3, \"Charlie\", 30),\n",
        "    (4, \"Diana\", 40),\n",
        "    (5, \"Eve\", 25)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n"
      ],
      "metadata": {
        "id": "XLSxJMAQiXv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chained filter with &, |, ~\n",
        "filtered_df = df.filter(\n",
        "    ((col(\"age\") > 30) & (col(\"name\") != \"Diana\")) | (~(col(\"age\") < 30))\n",
        ")\n",
        "\n",
        "# Show result\n",
        "filtered_df.show()\n"
      ],
      "metadata": {
        "id": "OC2RUiBtkw6f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}