{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os"
      ],
      "metadata": {
        "id": "qBF6I2YkkAVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743d5cb0-ff44-40f1-fcbb-1cfbc458b02e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk -y\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "FUT6gZlMkAgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aadf18e6-a6d9-4793-c6bd-66360d36e741"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Waiting for headers] [4 InRelease 12.7 kB/129 kB 10%] [Waiting for headers]\r                                                                               \rGet:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "\r0% [Waiting for headers] [4 InRelease 14.2 kB/129 kB 11%] [5 InRelease 6,555 B/\r0% [Waiting for headers] [4 InRelease 14.2 kB/129 kB 11%] [Waiting for headers]\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,702 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,968 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,402 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,728 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.3 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,264 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,951 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,564 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n",
            "Fetched 31.8 MB in 3s (9,953 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxxf86dga1 openjdk-11-jre x11-utils\n",
            "Suggested packages:\n",
            "  libxt-doc openjdk-11-demo openjdk-11-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libxt-dev libxxf86dga1 openjdk-11-jdk openjdk-11-jre\n",
            "  x11-utils\n",
            "0 upgraded, 9 newly installed, 0 to remove and 47 not upgraded.\n",
            "Need to get 6,907 kB of archives.\n",
            "After this operation, 16.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre amd64 11.0.27+6~us1-0ubuntu1~22.04 [214 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk amd64 11.0.27+6~us1-0ubuntu1~22.04 [2,895 kB]\n",
            "Fetched 6,907 kB in 15s (466 kB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126458 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../1-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../2-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../3-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../4-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../5-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../6-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-11-jre:amd64.\n",
            "Preparing to unpack .../7-openjdk-11-jre_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-11-jdk:amd64.\n",
            "Preparing to unpack .../8-openjdk-11-jdk_11.0.27+6~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up openjdk-11-jre:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up openjdk-11-jdk:amd64 (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.11/dist-packages/pyspark\""
      ],
      "metadata": {
        "id": "iGA_q1bFkAjV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ez6RtB7kAmV",
        "outputId": "280e8ece-aa28-41db-cdf2-a21f70dfe943"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bring in Pyspark functions into your session\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "p6foQr3xk0at"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start our Pyspark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "su8z_OK4kjhl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "•\tReading and Writing Data"
      ],
      "metadata": {
        "id": "aZFygDc1FiDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create simple dataframe\n",
        "df = spark.createDataFrame([(1, \"foo\"), (2, \"bar\"), (1, \"foo\")], [\"id\", \"value\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPKFDTy-kApF",
        "outputId": "d2d54597-02e0-4c63-db33-e94ef5ba36eb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id|value|\n",
            "+---+-----+\n",
            "|  1|  foo|\n",
            "|  2|  bar|\n",
            "|  1|  foo|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mydata = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"original.csv\")"
      ],
      "metadata": {
        "id": "OjkIo5vxkAsF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import json file\n",
        "\n",
        "import json\n",
        "df2 = spark.read.json('people.json')\n",
        "df2.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm_PgHUrFmJ8",
        "outputId": "efbc183d-9397-4129-ebe9-ff80bbd9a5ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------+\n",
            "| age|   name|\n",
            "+----+-------+\n",
            "|NULL|Michael|\n",
            "|  30|   Andy|\n",
            "|  19| Justin|\n",
            "+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HB-ZU3NOk7rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Checking/cleaning & *Validation*"
      ],
      "metadata": {
        "id": "znygJjt1MtmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV3UzBWidxor",
        "outputId": "3f24d495-6295-44f5-ba0c-3236e64b9865"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- JobTitle: string (nullable = true)\n",
            " |-- Salary: string (nullable = true)\n",
            " |-- Latitude: string (nullable = true)\n",
            " |-- Longitude: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploratory Functions\n",
        "\n",
        "\n",
        "df.describe()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgEE6Qm-FYOG",
        "outputId": "9b2c8b56-c2d7-4299-9828-acdc1ce040d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, id: string, value: string]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqaI54bf6XnN",
        "outputId": "2ed9ed46-5086-41ae-94e2-ad3dea25b721"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, id: string, value: string]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mydata.columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMPiQ5X06XYN",
        "outputId": "857136bf-c6d1-468a-ab88-00682dcafe16"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'first_name',\n",
              " 'last_name',\n",
              " 'gender',\n",
              " 'City',\n",
              " 'JobTitle',\n",
              " 'Salary',\n",
              " 'Latitude',\n",
              " 'Longitude']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.dtypes"
      ],
      "metadata": {
        "id": "eKoN4TrAk7ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e07db4-4f27-4799-fef3-0c676ed0cad3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('age', 'bigint'), ('name', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Null Checking & Missing Value Analysis\n",
        "mydata.select([\n",
        "    sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
        "    for c in mydata.columns\n",
        "]).show()\n",
        "\n",
        "\n",
        "mydata.filter(mydata.col.isNull())\n",
        "#Drop Rows with Any Null\n",
        "mydata.na.drop()\n",
        "mydata.na.fill(0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B5T0_-QNk7jV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "666cca64-5291-4999-daa2-0e334cb4513e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+---------+------+----+--------+------+--------+---------+\n",
            "| id|first_name|last_name|gender|City|JobTitle|Salary|Latitude|Longitude|\n",
            "+---+----------+---------+------+----+--------+------+--------+---------+\n",
            "|  0|         0|        0|     0|   1|       2|     0|       1|        0|\n",
            "+---+----------+---------+------+----+--------+------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace Nulls Only in Selected Columns\n",
        "\n",
        "df_filled = df2.fillna(0, subset=['age'])\n",
        "df_filled .show()"
      ],
      "metadata": {
        "id": "yHQYNeWLk7f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48aa2085-13c5-4363-dc40-a44bda8a4615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "|age|   name|\n",
            "+---+-------+\n",
            "|  0|Michael|\n",
            "| 30|   Andy|\n",
            "| 19| Justin|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Duplicate Detection\n",
        "\n",
        "\n",
        "df.groupBy(df.columns) \\\n",
        "  .count() \\\n",
        "  .filter(\"count > 1\") \\\n",
        "  .show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df.dropDuplicates().show()\n",
        "\n",
        "df_drop= df.dropDuplicates()\n",
        "df_drop.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqJQ3rcZM126",
        "outputId": "ceeff033-9770-4dad-cc4d-0704537d30c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id|value|\n",
            "+---+-----+\n",
            "|  1|  foo|\n",
            "|  2|  bar|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rp6pUzVUM10C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEskwJLYM1pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Mining & Feature Engineering"
      ],
      "metadata": {
        "id": "dDlvR1EINH-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering and Conditions\n",
        "filter()\n",
        "\n"
      ],
      "metadata": {
        "id": "io69HNNENMLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chained conditions using &, |, ~"
      ],
      "metadata": {
        "id": "nk9LgqOUh_HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n"
      ],
      "metadata": {
        "id": "dGzT1mMrNO2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Alice\", 29),\n",
        "    (2, \"Bob\", 35),\n",
        "    (3, \"Eve\", 25),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
        "\n",
        "# Simple condition: age > 30 OR name == \"Eve\"\n",
        "filtered_df = df.filter((col(\"age\") > 30) | (col(\"name\") == \"Eve\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "LUfMtOuMiTj8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df.show()"
      ],
      "metadata": {
        "id": "VZdHFgGViWpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6031babc-0843-4414-8cb9-a7b97afc1a74"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  2| Bob| 35|\n",
            "|  3| Eve| 25|\n",
            "+---+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark, you can use chained conditions with the bitwise operators:\n",
        "•\t& for AND\n",
        "•\t| for OR\n",
        "•\t~ for NOT\n",
        "These are used inside filter() or where() clauses, and each condition must be enclosed in parentheses to avoid operator precedence issues.\n"
      ],
      "metadata": {
        "id": "e5GTuqtrkmEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n"
      ],
      "metadata": {
        "id": "6eqghLAQiX4w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    (1, \"Alice\", 29),\n",
        "    (2, \"Bob\", 35),\n",
        "    (3, \"Charlie\", 30),\n",
        "    (4, \"Diana\", 40),\n",
        "    (5, \"Eve\", 25)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "XLSxJMAQiXv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05891a2e-bcf1-4784-b5ae-422ab08fac55"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 29|\n",
            "|  2|    Bob| 35|\n",
            "|  3|Charlie| 30|\n",
            "|  4|  Diana| 40|\n",
            "|  5|    Eve| 25|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chained filter with &, |, ~\n",
        "# Show me those who are over 30 years of age and excluded anyone named Diana,\n",
        "filtered_df = df.filter(\n",
        "    ((col(\"age\") > 30) & (col(\"name\") != \"Diana\"))\n",
        ")\n",
        "\n",
        "# Show result\n",
        "filtered_df.show()\n",
        "\n",
        "filtered_df = df.filter(\n",
        "    ((col(\"age\") > 30) & (col(\"name\") != \"Diana\")) | (~(col(\"age\") < 30))\n",
        ")\n",
        "\n",
        "# Show result\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "id": "OC2RUiBtkw6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2116612-da28-4097-f6ca-a14db87bf144"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  2| Bob| 35|\n",
            "+---+----+---+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
